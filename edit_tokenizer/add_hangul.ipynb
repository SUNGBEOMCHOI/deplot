{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../final_model/tokenizer.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "vocab_list = json_data['model']['vocab']\n",
    "new_json_data = copy.deepcopy(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50344it [00:00, 2014616.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# value에 따라 vocab_list 복사하여 정렬\n",
    "sorted_vocab = sorted(vocab_list, key=lambda x: x[1])\n",
    "\n",
    "# 가~힣까지의 유니코드 범위\n",
    "start = 0xAC00\n",
    "end = 0xD7A3\n",
    "\n",
    "# sorted_vocab의 문자들을 한글로 대체\n",
    "replacement_dict = {}\n",
    "for idx, (char, value) in enumerate(sorted_vocab):\n",
    "    if start > end:\n",
    "        break\n",
    "    replacement_dict[char] = chr(start)\n",
    "    start += 1\n",
    "\n",
    "# 원래의 vocab_list에서 문자 값을 replacement_dict에서 찾아 한글로 대체\n",
    "for idx, (char, value) in tqdm.tqdm(enumerate(vocab_list)):\n",
    "    if char in replacement_dict:\n",
    "        vocab_list[idx][0] = replacement_dict[char]\n",
    "\n",
    "new_json_data['model']['vocab'] = vocab_list\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('../final_model/new_tokenizer.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(new_json_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '''서울\n",
    "##주고\n",
    "##장은\n",
    "##상을\n",
    "얼마\n",
    "들어가\n",
    "##이어\n",
    "상품\n",
    "먼저\n",
    "더욱\n",
    "자연\n",
    "일본\n",
    "세계\n",
    "##an\n",
    "아래\n",
    "##리에\n",
    "않은\n",
    "##거든\n",
    "##들과\n",
    "여름\n",
    "##트를\n",
    "##된다\n",
    "##전에\n",
    "오래\n",
    "가지고\n",
    "작은\n",
    "##하니\n",
    "그렇\n",
    "##감이\n",
    "이야\n",
    "##만원\n",
    "올라\n",
    "##정을\n",
    "##이를\n",
    "있다고\n",
    "지역\n",
    "그래도\n",
    "공간\n",
    "사랑\n",
    "항상\n",
    "대해\n",
    "얼굴\n",
    "기본\n",
    "아기\n",
    "이것\n",
    "00\n",
    "운동\n",
    "하면\n",
    "제작\n",
    "그러나\n",
    "현재\n",
    "화장\n",
    "됩니다\n",
    "##거든요\n",
    "상황\n",
    "##지요\n",
    "전문\n",
    "어느\n",
    "##동안\n",
    "기대\n",
    "대표\n",
    "좋을\n",
    "없다\n",
    "이어\n",
    "자주\n",
    "##인이\n",
    "추가\n",
    "##셔서\n",
    "보내\n",
    "##정이\n",
    "거의\n",
    "##했습니다\n",
    "고민\n",
    "##리어\n",
    "해요\n",
    "구성\n",
    "것도\n",
    "일단\n",
    "활용\n",
    "않았\n",
    "말했다\n",
    "있다는\n",
    "좋고\n",
    "느낌이\n",
    "##or\n",
    "##식을\n",
    "수도\n",
    "##만큼\n",
    "국내\n",
    "살짝\n",
    "브랜드\n",
    "##상이\n",
    "이름\n",
    "어떻\n",
    "##화를\n",
    "개인\n",
    "##수가\n",
    "##ar\n",
    "관련\n",
    "##마트\n",
    "사용하\n",
    "중국\n",
    "##라서\n",
    "내용\n",
    "##해도\n",
    "전체\n",
    "보이\n",
    "##라인\n",
    "##가능\n",
    "여행\n",
    "##제로\n",
    "포장\n",
    "어떻게\n",
    "근데\n",
    "알려\n",
    "지나\n",
    "포스\n",
    "##지막\n",
    "##스타\n",
    "그런데\n",
    "기존\n",
    "해야\n",
    "연결\n",
    "##같은\n",
    "##스로\n",
    "##을때\n",
    "##자를\n",
    "##이블\n",
    "쉽게\n",
    "그렇게\n",
    "##르게\n",
    "##운데\n",
    "18\n",
    "##중에\n",
    "도움\n",
    "##도가\n",
    "지원\n",
    "느껴\n",
    "위치\n",
    "##이었다\n",
    "##at\n",
    "##까요\n",
    "##en\n",
    "청소\n",
    "##사가\n",
    "아침\n",
    "만나\n",
    "등을\n",
    "##하는데\n",
    "결과\n",
    "관계\n",
    "##겠어요\n",
    "##졌다\n",
    "간단\n",
    "해주\n",
    "그는\n",
    "받아\n",
    "##시에\n",
    "##하지만\n",
    "##겠다\n",
    "##이랑\n",
    "##져서\n",
    "알아\n",
    "##스가\n",
    "##르는\n",
    "제공\n",
    "매우\n",
    "##게요\n",
    "만든\n",
    "자동\n",
    "사회\n",
    "먹을\n",
    "##리지\n",
    "먹는\n",
    "촉촉\n",
    "##al\n",
    "어려\n",
    "##워서\n",
    "##자의\n",
    "그것\n",
    "어디\n",
    "##크림\n",
    "##0년\n",
    "##드를\n",
    "##양이\n",
    "##는지\n",
    "예쁘\n",
    "착용\n",
    "겨울\n",
    "약간\n",
    "최근\n",
    "무엇\n",
    "마지막\n",
    "방문\n",
    "나온\n",
    "##아지\n",
    "가지\n",
    "동안\n",
    "##나요\n",
    "힘들\n",
    "##겠지만\n",
    "##es\n",
    "제일\n",
    "##시키\n",
    "않아\n",
    "완성\n",
    "고객\n",
    "확실\n",
    "기술\n",
    "##르면\n",
    "##어도\n",
    "##하세요\n",
    "여성\n",
    "않을\n",
    "건조\n",
    "##으나\n",
    "전에\n",
    "##머니\n",
    "##도를\n",
    "##ed\n",
    "##이지만\n",
    "관리\n",
    "##구매\n",
    "##주의\n",
    "오늘은\n",
    "정보\n",
    "##이드\n",
    "최대\n",
    "깨끗\n",
    "발생\n",
    "참고\n",
    "가족\n",
    "있지만\n",
    "##장에\n",
    "자세\n",
    "##학교\n",
    "##리아\n",
    "##려서\n",
    "##라도\n",
    "새로운\n",
    "이거\n",
    "차량\n",
    "인터\n",
    "관심\n",
    "걱정\n",
    "정부\n",
    "커피\n",
    "##문에\n",
    "마음에\n",
    "자체\n",
    "기업\n",
    "사업\n",
    "##지않\n",
    "했는데\n",
    "필요한\n",
    "포스팅\n",
    "어린\n",
    "##으로도\n",
    "위해서\n",
    "나타\n",
    "##치를\n",
    "교육\n",
    "##인트\n",
    "나와\n",
    "좋아하는\n",
    "제거\n",
    "제품을\n",
    "##겠죠\n",
    "비교\n",
    "머리\n",
    "블랙\n",
    "언제\n",
    "게임\n",
    "스타일\n",
    "##번째\n",
    "##이며\n",
    "##감을\n",
    "사용하는\n",
    "##스는\n",
    "부담\n",
    "##개월\n",
    "##나는\n",
    "##인데요\n",
    "##장에서\n",
    "따로\n",
    "감사\n",
    "##해드\n",
    "16\n",
    "쓰고\n",
    "높은\n",
    "##오는\n",
    "##도로\n",
    "냄새\n",
    "유지\n",
    "있도록\n",
    "##물을\n",
    "그대로\n",
    "##치는\n",
    "##그램\n",
    "원래\n",
    "좋습니다\n",
    "시간이\n",
    "##스러운\n",
    "##구나\n",
    "영화\n",
    "있으니\n",
    "시원\n",
    "##주면\n",
    "##데요\n",
    "##물이\n",
    "바람\n",
    "##원이\n",
    "##수를\n",
    "##시켜\n",
    "##하나\n",
    "##하였\n",
    "알고\n",
    "그림\n",
    "##립니다\n",
    "에어\n",
    "##사이\n",
    "크게\n",
    "유명\n",
    "##심히\n",
    "기다\n",
    "##있어요\n",
    "50\n",
    "##므로\n",
    "##하시\n",
    "대부분\n",
    "##하시는\n",
    "색상\n",
    "활동\n",
    "하지\n",
    "집에\n",
    "고급\n",
    "제품이\n",
    "결국\n",
    "제대로\n",
    "음식\n",
    "모델\n",
    "되고\n",
    "싶은\n",
    "포함\n",
    "충분\n",
    "나왔\n",
    "내려\n",
    "또는\n",
    "서비\n",
    "14\n",
    "이러\n",
    "정도로\n",
    "올려\n",
    "좋네요'''\n",
    "b = a.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [char for char in b if '#' not in char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replacement_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50344it [00:00, 1230080.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import json\n",
    "\n",
    "# 주어진 대체할 한글\n",
    "replacement_chars = ['최대', '증', '에', '업', '토', '보', '청', '함', '루', '그', '지', '남', '로', '을', '포', '레', '만', '작', '크', '리', '자', '디', '했', '도', '천', '게', '용', '기', '어', '술', '야', '월', '단', '겠', '행', '악', '네', '유', '는', '피', '치', '학', '티', '건', '버', '시', '으', '여', '송', '합', '소', '져', '속', '즈', '각', '집', '린', '부', '점', '교', '화', '사', '금', '민', '인', '종', '통', '질', '내', '권', '중', '급', '코', '트', '심', '회', '데', '실', '며', '물', '서', '가', '안', '명', '된', '를', '브', '선', '체', '조', '품', '면', '온', '전', '발', '우', '태', '일', '구', '키', '준', '이', '진', '문', '연', '역', '움', '운', '매', '두', '터', '프', '후', '입', '형', '번', '결', '래', '현', '더', '간', '카', '란', '다', '와', '려', '거', '복', '임', '신', '되', '주', '표', '미', '히', '관', '할', '경', '격', '절', '스', '고', '족', '당', '산', '국', '모', '식', '대', '니', '요', '테', '별', '저', '정', '년', '직', '수', '공', '군', '던', '분', '설', '비', '위', '출', '원', '르', '상', '나', '생', '양', '파', '아', '색', '론', '반', '든', '라', '망', '무', '한', '약', '음', '석', '바', '의', '차', '장', '노', '세', '적']  # 이 부분에 주어진 한글 문자를 전부 넣으세요\n",
    "new_chars = b\n",
    "# 대체할 숫자 생성\n",
    "done_list = []\n",
    "base = 10\n",
    "# while len(numbers) < len(replacement_chars):\n",
    "#     for i in range(1, 10):\n",
    "#         numbers.append(i * base)\n",
    "#     base *= 10\n",
    "\n",
    "# 대체 사전 생성\n",
    "replacement_dict = dict(zip(replacement_chars, new_chars))\n",
    "\n",
    "# 원래의 vocab_list에서 문자 값을 replacement_dict에서 찾아 숫자로 대체\n",
    "for idx, (char, value) in tqdm.tqdm(enumerate(vocab_list)):\n",
    "    if char in replacement_dict and char not in done_list:\n",
    "        vocab_list[idx][0] = replacement_dict[char]\n",
    "        done_list.append(char)\n",
    "\n",
    "new_json_data['model']['vocab'] = vocab_list\n",
    "\n",
    "# # JSON 파일로 저장\n",
    "with open('../final_model/new_tokenizer.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(new_json_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 191kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 344k/344k [00:00<00:00, 501kB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 725/725 [00:00<00:00, 189kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_dict['성']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check hangul data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../final_model/new_tokenizer.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "vocab_list = json_data['model']['vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# vocab_list에서 문자만 추출\n",
    "vocab_chars = [char for char, value in vocab_list]\n",
    "\n",
    "# 가~힣까지의 유니코드 범위\n",
    "start = 0xAC00\n",
    "end = 0xD7A3\n",
    "\n",
    "# vocab_list에 없는 한글 문자 찾기\n",
    "missing_chars = []\n",
    "for code in range(start, end+1):\n",
    "    char = chr(code)\n",
    "    if char not in vocab_chars:\n",
    "        missing_chars.append(char)\n",
    "\n",
    "# 중복 문자 확인\n",
    "duplicates = [char for char in set(vocab_chars) if vocab_chars.count(char) > 1]\n",
    "\n",
    "print(missing_chars)\n",
    "print(duplicates)  # 중복 문자 출력\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
